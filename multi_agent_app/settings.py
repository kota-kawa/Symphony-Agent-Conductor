"""Helpers for loading and saving operator-managed settings."""

from __future__ import annotations

import hashlib
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Iterable

from .config import ORCHESTRATOR_MODEL

DEFAULT_AGENT_CONNECTIONS: Dict[str, bool] = {
    "lifestyle": True,
    "browser": True,
    "iot": True,
    "scheduler": True,
}

DEFAULT_MODEL_SELECTIONS: Dict[str, Dict[str, str]] = {
    "orchestrator": {"provider": "groq", "model": ORCHESTRATOR_MODEL, "base_url": ""},
    "browser": {"provider": "groq", "model": "openai/gpt-oss-20b", "base_url": ""},
    "lifestyle": {"provider": "groq", "model": "openai/gpt-oss-20b", "base_url": ""},
    "iot": {"provider": "groq", "model": "openai/gpt-oss-20b", "base_url": ""},
    "scheduler": {"provider": "groq", "model": "openai/gpt-oss-20b", "base_url": ""},
    "memory": {"provider": "groq", "model": ORCHESTRATOR_MODEL, "base_url": ""},
}

DEFAULT_MEMORY_SETTINGS: Dict[str, Any] = {
    "enabled": True,
    "short_term_ttl_minutes": 45,
    "short_term_grace_minutes": 10,
    "short_term_active_task_hold_minutes": 20,
    "short_term_promote_score": 2,
    "short_term_promote_importance": 0.65,
}

LLM_PROVIDERS: Dict[str, Dict[str, Any]] = {
    "openai": {
        "label": "OpenAI",
        "api_key_env": "OPENAI_API_KEY",
        "base_url_env": "OPENAI_BASE_URL",
        "default_base_url": None,
        "models": [
            {"id": "gpt-5.1", "label": "GPT-5.1"},
        ],
    },
    "gemini": {
        "label": "Gemini (Google)",
        "api_key_env": "GEMINI_API_KEY",
        "base_url_env": "GEMINI_API_BASE",
        "default_base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "models": [
            {"id": "gemini-2.5-flash-lite", "label": "Gemini 2.5 Flash-Lite"},
            {"id": "gemini-3-pro-preview", "label": "Gemini 3 Pro Preview"},
        ],
    },
    "claude": {
        "label": "Claude (Anthropic)",
        "api_key_env": "ANTHROPIC_API_KEY",
        "api_key_env_aliases": ["CLAUDE_API_KEY"],
        "base_url_env": "ANTHROPIC_API_BASE",
        "base_url_env_aliases": ["CLAUDE_API_BASE"],
        "default_base_url": None,
        "models": [
            {"id": "claude-haiku-4-5", "label": "Claude Haiku 4.5"},
            {"id": "claude-opus-4-5", "label": "Claude Opus 4.5"},
        ],
    },
    "groq": {
        "label": "Groq",
        "api_key_env": "GROQ_API_KEY",
        "base_url_env": "GROQ_API_BASE",
        "default_base_url": "https://api.groq.com/openai/v1",
        "models": [
            {"id": "llama-3.3-70b-versatile", "label": "Llama 3.3 70B (Groq)"},
            {"id": "llama-3.1-8b-instant", "label": "Llama 3.1 8B (Groq)"},
            {"id": "openai/gpt-oss-20b", "label": "GPT-OSS 20B (Groq)"},
            {"id": "qwen/qwen3-32b", "label": "Qwen3 32B (Groq)"},
        ],
    },
}

_AGENT_CONNECTIONS_FILE = "agent_connections.json"
_MODEL_SETTINGS_FILE = "model_settings.json"
_MEMORY_SETTINGS_FILE = "memory_settings.json"
_REPO_ROOT = Path(__file__).resolve().parents[2]
_AGENT_ENV_PATHS: Dict[str, Path] = {
    "orchestrator": _REPO_ROOT / "Multi-Agent-Platform" / "secrets.env",
    "browser": _REPO_ROOT / "Browser-Agent" / "secrets.env",
    "lifestyle": _REPO_ROOT / "Life-Style-Agent" / "secrets.env",
    "iot": _REPO_ROOT / "IoT-Agent" / "secrets.env",
    "scheduler": _REPO_ROOT / "Scheduler-Agent" / "secrets.env",
    "memory": _REPO_ROOT / "Multi-Agent-Platform" / "secrets.env",
}


def _coerce_bool(value: Any, fallback: bool) -> bool:
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        lowered = value.strip().lower()
        if lowered in {"1", "true", "yes", "on"}:
            return True
        if lowered in {"0", "false", "no", "off"}:
            return False
    if isinstance(value, (int, float)):
        return bool(value)
    return fallback


def _coerce_int(value: Any, fallback: int, *, minimum: int | None = None, maximum: int | None = None) -> int:
    candidate: int | None = None
    if isinstance(value, int):
        candidate = value
    elif isinstance(value, float):
        candidate = int(value)
    elif isinstance(value, str) and value.strip():
        try:
            candidate = int(float(value))
        except ValueError:
            candidate = None

    if candidate is None:
        candidate = fallback

    if minimum is not None:
        candidate = max(minimum, candidate)
    if maximum is not None:
        candidate = min(maximum, candidate)
    return candidate


def _coerce_float(value: Any, fallback: float, *, minimum: float | None = None, maximum: float | None = None) -> float:
    candidate: float | None = None
    if isinstance(value, (int, float)):
        candidate = float(value)
    elif isinstance(value, str) and value.strip():
        try:
            candidate = float(value)
        except ValueError:
            candidate = None

    if candidate is None:
        candidate = fallback

    if minimum is not None:
        candidate = max(minimum, candidate)
    if maximum is not None:
        candidate = min(maximum, candidate)
    return candidate


def _merge_connections(raw: Any) -> Dict[str, bool]:
    merged = dict(DEFAULT_AGENT_CONNECTIONS)
    if not isinstance(raw, dict):
        return merged
    source = raw.get("agents") if "agents" in raw else raw
    if not isinstance(source, dict):
        return merged

    for key, default_value in DEFAULT_AGENT_CONNECTIONS.items():
        merged[key] = _coerce_bool(source.get(key), default_value)
    return merged


def load_agent_connections() -> Dict[str, bool]:
    """Load the on/off state for each agent. Defaults to all enabled."""
    try:
        with open(_AGENT_CONNECTIONS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return dict(DEFAULT_AGENT_CONNECTIONS)

    return _merge_connections(data)


def save_agent_connections(payload: Dict[str, Any]) -> Dict[str, bool]:
    """Persist the agent connection toggles to disk."""
    connections = _merge_connections(payload)
    with open(_AGENT_CONNECTIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(connections, f, ensure_ascii=False, indent=2)
    return connections


def _read_env_file(path: Path) -> Dict[str, str]:
    """Parse a simple KEY=VALUE env file into a dict."""

    values: Dict[str, str] = {}
    try:
        content = path.read_text(encoding="utf-8")
    except OSError:
        return values

    for raw_line in content.splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        if not key:
            continue
        cleaned = value.strip()
        if (
            (cleaned.startswith('"') and cleaned.endswith('"'))
            or (cleaned.startswith("'") and cleaned.endswith("'"))
        ):
            cleaned = cleaned[1:-1]
        values[key] = cleaned
    return values


def _load_agent_env(agent: str) -> Dict[str, str]:
    """Return a merged view of environment variables for the given agent."""

    env: Dict[str, str] = {key: value for key, value in os.environ.items()}
    env_path = _AGENT_ENV_PATHS.get(agent)
    if env_path:
        env.update(_read_env_file(env_path))
    return env


def _pick_env_value(env: Dict[str, str], names: Iterable[str]) -> str | None:
    """Return the first non-empty env value for the given list of names."""

    for name in names:
        if not name:
            continue
        for candidate in (name, name.lower()):
            value = env.get(candidate)
            if value is None:
                continue
            cleaned = str(value).strip()
            if cleaned:
                return cleaned
    return None


def _merge_model_selection(raw: Any) -> Dict[str, Dict[str, str]]:
    """Coerce user-provided model selection into a safe structure."""

    merged = dict(DEFAULT_MODEL_SELECTIONS)
    if not isinstance(raw, dict):
        return merged

    source = raw.get("selection") if "selection" in raw else raw
    if not isinstance(source, dict):
        return merged

    for agent, default_selection in DEFAULT_MODEL_SELECTIONS.items():
        value = source.get(agent) if isinstance(source.get(agent), dict) else {}
        provider = (value.get("provider") or default_selection["provider"]).strip()
        model = (value.get("model") or default_selection["model"]).strip()
        base_url = (value.get("base_url") or default_selection.get("base_url", "") or "").strip()
        provider_meta = LLM_PROVIDERS.get(provider)
        provider_default_base = str(provider_meta.get("default_base_url") or "").strip() if provider_meta else ""
        if not base_url and provider_default_base:
            base_url = provider_default_base

        if not provider_meta:
            merged[agent] = dict(default_selection)
            continue

        valid_models = {m["id"] for m in provider_meta.get("models", [])}
        if model not in valid_models:
            merged[agent] = dict(default_selection)
            continue

        merged[agent] = {"provider": provider, "model": model, "base_url": base_url}
    return merged


def load_model_settings() -> Dict[str, Dict[str, str]]:
    """Load the selected LLM per agent."""

    try:
        with open(_MODEL_SETTINGS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return dict(DEFAULT_MODEL_SELECTIONS)

    return _merge_model_selection(data)


def save_model_settings(payload: Dict[str, Any]) -> Dict[str, Dict[str, str]]:
    """Persist the model selections to disk."""

    existing = load_model_settings()
    incoming = payload.get("selection") if isinstance(payload, dict) and "selection" in payload else payload

    merged_input: Dict[str, Dict[str, str]] = dict(existing)
    if isinstance(incoming, dict):
        for agent, value in incoming.items():
            if agent not in DEFAULT_MODEL_SELECTIONS or not isinstance(value, dict):
                continue
            merged_input[agent] = {**existing.get(agent, {}), **value}

    selection = _merge_model_selection(merged_input)
    with open(_MODEL_SETTINGS_FILE, "w", encoding="utf-8") as f:
        json.dump(selection, f, ensure_ascii=False, indent=2)
    return selection


def _normalize_memory_settings(raw: Dict[str, Any] | None) -> Dict[str, Any]:
    merged: Dict[str, Any] = dict(DEFAULT_MEMORY_SETTINGS)

    merged["enabled"] = _coerce_bool(raw.get("enabled") if raw else None, merged["enabled"])
    merged["short_term_ttl_minutes"] = _coerce_int(
        raw.get("short_term_ttl_minutes") if raw else None,
        merged["short_term_ttl_minutes"],
        minimum=5,
        maximum=720,
    )
    merged["short_term_grace_minutes"] = _coerce_int(
        raw.get("short_term_grace_minutes") if raw else None,
        merged["short_term_grace_minutes"],
        minimum=0,
        maximum=240,
    )
    merged["short_term_active_task_hold_minutes"] = _coerce_int(
        raw.get("short_term_active_task_hold_minutes") if raw else None,
        merged["short_term_active_task_hold_minutes"],
        minimum=0,
        maximum=240,
    )
    merged["short_term_promote_score"] = _coerce_int(
        raw.get("short_term_promote_score") if raw else None,
        merged["short_term_promote_score"],
        minimum=0,
        maximum=10,
    )
    merged["short_term_promote_importance"] = _coerce_float(
        raw.get("short_term_promote_importance") if raw else None,
        merged["short_term_promote_importance"],
        minimum=0.0,
        maximum=1.0,
    )
    return merged


def load_memory_settings() -> Dict[str, Any]:
    """Load the memory usage settings."""
    try:
        with open(_MEMORY_SETTINGS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return dict(DEFAULT_MEMORY_SETTINGS)

    return _normalize_memory_settings(data if isinstance(data, dict) else {})


def save_memory_settings(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Persist the memory usage settings."""
    existing = load_memory_settings()
    incoming = payload if isinstance(payload, dict) else {}

    merged = dict(existing)
    for key in DEFAULT_MEMORY_SETTINGS:
        if key not in incoming:
            continue
        value = incoming[key]
        if value is None:
            continue
        merged[key] = value

    settings = _normalize_memory_settings(merged)
    with open(_MEMORY_SETTINGS_FILE, "w", encoding="utf-8") as f:
        json.dump(settings, f, ensure_ascii=False, indent=2)
    return settings


def get_llm_options() -> Dict[str, List[Dict[str, Any]]]:
    """Expose provider/model options for the UI."""

    providers: List[Dict[str, Any]] = []
    for provider_id, meta in LLM_PROVIDERS.items():
        providers.append(
            {
                "id": provider_id,
                "label": meta.get("label") or provider_id,
                "models": meta.get("models", []),
            },
        )
    return {"providers": providers}


def resolve_llm_config(agent: str) -> Dict[str, Any]:
    """Return ChatOpenAI-ready config for the given agent's selected model."""

    selection = load_model_settings().get(agent) or DEFAULT_MODEL_SELECTIONS.get(agent)
    if not selection:
        raise ValueError(f"Unknown agent '{agent}' for model resolution.")

    provider_id = selection.get("provider") or ""
    provider_meta = LLM_PROVIDERS.get(provider_id)
    if not provider_meta:
        raise ValueError(f"Unsupported provider '{provider_id}'.")

    model_name = selection.get("model")
    valid_models = {m["id"] for m in provider_meta.get("models", [])}
    if not model_name or model_name not in valid_models:
        raise ValueError(f"モデル '{model_name}' はプロバイダー '{provider_id}' では利用できません。")

    env = _load_agent_env(agent)
    api_key_name = provider_meta.get("api_key_env") or "OPENAI_API_KEY"
    api_key_aliases = provider_meta.get("api_key_env_aliases") or []
    api_key = _pick_env_value(env, [api_key_name, *api_key_aliases])
    if not api_key:
        missing_key_hint = api_key_name
        if api_key_aliases:
            missing_key_hint = f"{api_key_name} または {', '.join(api_key_aliases)}"
        raise ValueError(f"{missing_key_hint} を {agent} の secrets.env に設定してください。")

    base_url_env = provider_meta.get("base_url_env")
    base_url_aliases = provider_meta.get("base_url_env_aliases") or []
    base_url_candidates: List[str] = []
    if base_url_env:
        base_url_candidates.append(base_url_env)
    base_url_candidates.extend(base_url_aliases)

    base_url = _pick_env_value(env, base_url_candidates) or provider_meta.get("default_base_url")

    key_fingerprint = hashlib.sha256(api_key.encode("utf-8")).hexdigest()[:12]
    return {
        "provider": provider_id,
        "model": model_name,
        "api_key": api_key,
        "base_url": base_url,
        "api_key_fingerprint": key_fingerprint,
    }


def validate_model_selection(payload: Dict[str, Any]) -> Dict[str, Dict[str, str]]:
    """Return the validated model selection without persisting it."""

    return _merge_model_selection(payload)
